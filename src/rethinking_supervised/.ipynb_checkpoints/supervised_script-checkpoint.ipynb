{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d50476d-3ff4-4ccd-b1c5-233c8c7659b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import socket\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import model_pool\n",
    "from models.util import create_model\n",
    "\n",
    "from dataset.mini_imagenet import ImageNet, MetaImageNet\n",
    "from dataset.mnist import MNIST, MetaMNIST\n",
    "from dataset.cifar import CIFAR100, MetaCIFAR100\n",
    "from dataset.transform_cfg import transforms_options, transforms_list\n",
    "\n",
    "from util import adjust_learning_rate, accuracy, AverageMeter\n",
    "from eval.meta_eval import meta_test\n",
    "from eval.cls_eval import validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6982e72b-137f-4a95-a1ca-c95316c716e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "parser.add_argument('--eval_freq', type=int, default=10, help='meta-eval frequency')\n",
    "parser.add_argument('--print_freq', type=int, default=100, help='print frequency')\n",
    "parser.add_argument('--tb_freq', type=int, default=500, help='tb frequency')\n",
    "parser.add_argument('--save_freq', type=int, default=10, help='save frequency')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch_size')\n",
    "parser.add_argument('--num_workers', type=int, default=8, help='num of workers to use')\n",
    "parser.add_argument('--epochs', type=int, default=200, help='number of training epochs')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n",
    "parser.add_argument('--lr_decay_epochs', type=str, default='60,80', help='where to decay lr, can be a list')\n",
    "parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate for learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--adam', action='store_true', help='use adam optimizer')\n",
    "\n",
    "# dataset\n",
    "parser.add_argument('--model', type=str, default='resnet50', choices=model_pool)\n",
    "parser.add_argument('--dataset', type=str, default='CIFAR-FS', choices=['miniImageNet', 'tieredImageNet',\n",
    "                                                                                'CIFAR-FS', 'FC100'])\n",
    "parser.add_argument('--transform', type=str, default='A', choices=transforms_list)\n",
    "parser.add_argument('--use_trainval', action='store_true', help='use trainval set')\n",
    "\n",
    "# cosine annealing\n",
    "parser.add_argument('--cosine', action='store_true', help='using cosine annealing')\n",
    "\n",
    "# specify folder\n",
    "parser.add_argument('--model_path', type=str, default='', help='path to save model')\n",
    "parser.add_argument('--tb_path', type=str, default='', help='path to tensorboard')\n",
    "parser.add_argument('--data_root', type=str, default='', help='path to data root')\n",
    "\n",
    "# meta setting\n",
    "parser.add_argument('--n_test_runs', type=int, default=600, metavar='N',help='Number of test runs')\n",
    "parser.add_argument('--n_ways', type=int, default=3, metavar='N',help='Number of classes for doing each classification run')\n",
    "parser.add_argument('--n_shots', type=int, default=1, metavar='N', help='Number of shots in test')\n",
    "parser.add_argument('--n_queries', type=int, default=15, metavar='N',help='Number of query in test')\n",
    "parser.add_argument('--n_aug_support_samples', default=5, type=int,help='The number of augmented samples for each meta test sample')\n",
    "parser.add_argument('--test_batch_size', type=int, default=1, metavar='test_batch_size',help='Size of test batch)')\n",
    "parser.add_argument('-t', '--trial', type=str, default='1', help='the experiment id')\n",
    "\n",
    "opt = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbcfedb-14c9-4f5e-8b90-d58470b35155",
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.dataset == 'CIFAR-FS' or opt.dataset == 'FC100':\n",
    "    opt.transform = 'D'\n",
    "\n",
    "if opt.use_trainval:\n",
    "    opt.trial = opt.trial + '_trainval'\n",
    "\n",
    "# set the path according to the environment\n",
    "if not opt.model_path:\n",
    "    opt.model_path = './models_pretrained'\n",
    "if not opt.tb_path:\n",
    "    opt.tb_path = './tensorboard'\n",
    "if not opt.data_root:\n",
    "    opt.data_root = '../../datasets/{}'.format(opt.dataset)\n",
    "else:\n",
    "    opt.data_root = '{}/{}'.format(opt.data_root, opt.dataset)\n",
    "opt.data_aug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980d4eb2-28aa-4db3-af86-816b8e2202f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = opt.lr_decay_epochs.split(',')\n",
    "opt.lr_decay_epochs = list([])\n",
    "for it in iterations:\n",
    "    opt.lr_decay_epochs.append(int(it))\n",
    "opt.model_name = '{}_{}_lr_{}_decay_{}_trans_{}'.format(opt.model, opt.dataset, opt.learning_rate,opt.weight_decay, opt.transform)\n",
    "\n",
    "if opt.cosine:\n",
    "    opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "if opt.adam:\n",
    "    opt.model_name = '{}_useAdam'.format(opt.model_name)\n",
    "\n",
    "opt.model_name = '{}_trial_{}'.format(opt.model_name, opt.trial)\n",
    "\n",
    "opt.n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aedb713-979f-4736-afb1-2121f1cc5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets\n",
    "train_partition = 'trainval' if opt.use_trainval else 'train'\n",
    "\n",
    "train_trans, test_trans = transforms_options['D']\n",
    "\n",
    "train_loader = DataLoader(CIFAR100(args=opt, partition=train_partition, transform=train_trans),\n",
    "                                  batch_size=opt.batch_size, shuffle=True, drop_last=True,\n",
    "                                  num_workers=opt.num_workers)\n",
    "val_loader = DataLoader(CIFAR100(args=opt, partition='train', transform=test_trans),\n",
    "                                batch_size=opt.batch_size // 2, shuffle=False, drop_last=False,\n",
    "                                num_workers=opt.num_workers // 2)\n",
    "if opt.use_trainval:\n",
    "    n_cls = 80\n",
    "else:\n",
    "    n_cls = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "711772e7-7f21-4564-81a8-484d0ef14955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model name : resnet50, dataset : CIFAR-FS\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = create_model(opt.model, n_cls, opt.dataset)\n",
    "\n",
    "print(f'>> model name : {opt.model}, dataset : {opt.dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb9ae20-e13c-40c7-be9d-3a3f7047baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "if opt.adam:\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=opt.learning_rate,weight_decay=0.0005)\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(),lr=opt.learning_rate,momentum=opt.momentum,weight_decay=opt.weight_decay)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if opt.n_gpu > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb3da96-754e-484b-b068-0ae9de96b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cosine annealing scheduler\n",
    "if opt.cosine:\n",
    "    eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.epochs, eta_min, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c54470c-9484-43ba-bc91-875ba760ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    return (predictions == targets).sum().float() / targets.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2900bf40-3f33-4195-8273-20dd00e8382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_loader, model, criterion, optimizer, opt):\n",
    "    \"\"\"One epoch training\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc1 = AverageMeter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, (input, target, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input = input.float()\n",
    "        if torch.cuda.is_available():\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        # ===================forward=====================\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        acc = accuracy(output, target)\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        acc1.update(acc.item(), input.size(0))\n",
    "\n",
    "        # ===================backward=====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ===================meters=====================\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # tensorboard logger\n",
    "        pass\n",
    "\n",
    "        # print info\n",
    "        if idx % opt.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'.format(\n",
    "                   epoch, idx, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=acc1))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    print(' * Acc@1 {top1.avg:.3f}='.format(top1=acc1))\n",
    "\n",
    "    return acc1.avg, losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95522687-e548-4df7-b4f4-c626f0b48a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(1, opt.epochs + 1):\n",
    "    if opt.cosine:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        adjust_learning_rate(epoch, opt, optimizer)\n",
    "    print(\"==> training...\")\n",
    "\n",
    "    time1 = time.time()\n",
    "    train_acc, train_loss = train(epoch, train_loader, model, criterion, optimizer, opt)\n",
    "    time2 = time.time()\n",
    "    print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "\n",
    "    print('train_acc', train_acc, epoch)\n",
    "    print('train_loss', train_loss, epoch)\n",
    "\n",
    "    test_acc, test_loss = validate(val_loader, model, criterion, opt)\n",
    "\n",
    "    print(f'[epoch {epoch}] test_acc : {test_acc}')\n",
    "    print(f'[epoch {epoch}] test_loss : {test_loss}')\n",
    "\n",
    "    # regular saving\n",
    "    if best_acc < test_acc :\n",
    "        best_acc = test_acc\n",
    "        print('==> Saving...')\n",
    "        save_file = os.path.join('./checkpoint/ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
    "        torch.save(model.state_dict(), save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76f1d98-e701-4b19-beed-5c2cd0978920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete ...\n"
     ]
    }
   ],
   "source": [
    "print('Complete ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e38da-503b-4752-be5f-fa5c63be1cf0",
   "metadata": {},
   "source": [
    "## Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c892fecb-edd1-4438-87bd-45f2ee5cf9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model name : resnet50, model_path : ./distil_checkpoint/ckpt_epoch_33.pth\n"
     ]
    }
   ],
   "source": [
    "# checkpoint model path check\n",
    "model_frozen = False\n",
    "\n",
    "\n",
    "if model_frozen : \n",
    "    model = create_model(opt.model, n_cls, opt.dataset)\n",
    "    model = model.cuda()\n",
    "    print('Experiment with Imagenet weight frozen')\n",
    "else :\n",
    "    #model_path = './checkpoint/ckpt_epoch_198.pth' # multi class classification\n",
    "    model_path = './distil_checkpoint/ckpt_epoch_33.pth' # Distill multi class classification\n",
    "    model = create_model(opt.model, n_cls, opt.dataset)\n",
    "    ckpt = torch.load(model_path)\n",
    "    corrected_dict = { k.replace('module.', ''): v for k, v in ckpt.items() } \n",
    "    model.load_state_dict(corrected_dict)\n",
    "    model = model.cuda()\n",
    "    print(f'>> model name : {opt.model}, model_path : {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b605771-bbe9-4e95-b785-fd7db8f398be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test datasets\n",
    "dataset_nm = 'cifarfs' # choose : [cifarfs, miniImagenet, mnist]\n",
    "\n",
    "train_trans, test_trans = transforms_options[opt.transform]\n",
    "\n",
    "if dataset_nm == 'cifarfs' :\n",
    "    opt.data_root = '../../datasets/CIFAR-FS'\n",
    "    meta_testloader = DataLoader(MetaCIFAR100(args=opt, partition='test',\n",
    "                                                  train_transform=train_trans,\n",
    "                                                  test_transform=test_trans),\n",
    "                                     batch_size=opt.test_batch_size, shuffle=False, drop_last=False,\n",
    "                                     num_workers=opt.num_workers)\n",
    "elif dataset_nm == 'miniImagenet' :\n",
    "    train_trans, test_trans = transforms_options['A']\n",
    "    opt.data_root = '../../datasets/miniImageNet/'\n",
    "    meta_testloader = DataLoader(MetaImageNet(args=opt, partition='test',\n",
    "                                                  train_transform=train_trans,\n",
    "                                                  test_transform=test_trans),\n",
    "                                     batch_size=opt.test_batch_size, shuffle=False, drop_last=False,\n",
    "                                     num_workers=opt.num_workers)\n",
    "elif dataset_nm == 'mnist' :\n",
    "    # double MNIST (for few shot learning)\n",
    "    opt.transfomr = 'A'\n",
    "    train_trans, test_trans = transforms_options['A']\n",
    "    opt.data_root = '../../datasets/double_mnist'\n",
    "    meta_testloader = DataLoader(MetaMNIST(args=opt, partition='test',\n",
    "                                                  train_transform=train_trans,\n",
    "                                                  test_transform=test_trans),\n",
    "                                     batch_size=opt.test_batch_size, shuffle=False, drop_last=False,\n",
    "                                     num_workers=opt.num_workers)\n",
    "else : \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39eaa047-e65c-4f49-8345-45e6bf91250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:13, 44.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.6492592592592592, Test Std : 0.01155689104706098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "test_acc, test_std = meta_test(model, meta_testloader)\n",
    "print(f'Test Accuracy : {test_acc}, Test Std : {test_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587a5fd-79b1-4cba-9807-f0716d4e963d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cbaa0b-9bee-4599-b1b4-f566a4cf077a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b4382-4648-40b8-85a7-ed98b4854938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f9369-3952-483b-8b09-7eb224473dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
